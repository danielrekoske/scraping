{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import win32com.client as win32 \n",
    "import requests\n",
    "import traceback\n",
    "\n",
    "global stopword_dict\n",
    "global punctuations\n",
    "global commodities_of_interests\n",
    "\n",
    "punctuations= ['!', '(', ')', '[', ']', '{', '}', ';', ':', \"'\", '\"',\n",
    "               '\\\\', ',', '<', '>', '.', '/', '?', '@', '#', '%', '^',\n",
    "               '&', '*', '_', '~',]\n",
    "\n",
    "#remove swearing words\n",
    "stopword_dict=punctuations+['i','yolo', 'fuck', 'fucking', 'shit',\n",
    "                            'take', 'still', 'new', 'say', 'get',\n",
    "                            'add', 'update', 'me', 'my', 'myself',\n",
    "                            'we', 'our', 'ours', 'ourselves', 'be',\n",
    "                            'you', \"you're\", \"you've\", \"you'll\",\n",
    "                            \"you'd\", 'your', 'yours', 'yourself',\n",
    "                            'yourselves', 'he', 'him', 'his', 'were',\n",
    "                            'himself', 'she', \"she's\", 'her', 'been',\n",
    "                            'hers', 'herself', 'it', \"it's\", 'being',\n",
    "                            'its', 'itself', 'they', 'them', 'their',\n",
    "                            'theirs', 'themselves', 'what', 'which',\n",
    "                            'who', 'whom', 'this', 'that', \"that'll\",\n",
    "                            'these', 'those', 'am', 'is', 'are', 'was',\n",
    "                            'have', 'has', 'had', 'having', 'do',\n",
    "                            'does', 'did', 'doing', 'a', 'an', 'the',\n",
    "                            'and', 'but', 'if', 'or', 'because', 'as',\n",
    "                            'until', 'while', 'of', 'at', 'by', 'for',\n",
    "                            'with', 'about', 'against', 'between',\n",
    "                            'into', 'through', 'during', 'before',\n",
    "                            'after', 'above', 'below', 'to', 'from',\n",
    "                            'up', 'down', 'in', 'out', 'on', 'off',\n",
    "                            'over', 'under', 'again', 'further',\n",
    "                            'then', 'once', 'here', 'there', 'when',\n",
    "                            'where', 'why', 'how', 'all', 'any',\n",
    "                            'both', 'each', 'few', 'more', 'most',\n",
    "                            'other', 'some', 'such', 'no', 'nor',\n",
    "                            'not', 'only', 'own', 'same', 'so', 'than',\n",
    "                            'too', 'very', 's', 't', 'can', 'will',\n",
    "                            'just', 'don', \"don't\", 'should', \n",
    "                            \"should've\", 'now', 'd', 'll', 'm', 'o',\n",
    "                            're', 've', 'y', 'ain', 'aren', \"aren't\",\n",
    "                            'couldn', \"couldn't\", 'didn', \"didn't\",\n",
    "                            'doesn', \"doesn't\", 'hadn', \"hadn't\",\n",
    "                            'hasn', \"hasn't\", 'haven', \"haven't\",\n",
    "                            'isn', \"isn't\", 'ma', 'mightn', \"mightn't\",\n",
    "                            'mustn', \"mustn't\", 'needn', \"needn't\",\n",
    "                            'shan', \"shan't\", 'shouldn', \"shouldn't\",\n",
    "                             'wasn', \"wasn't\", 'weren', \"weren't\",\n",
    "                             'won','other','others', \"won't\",'guys',\n",
    "                             'another','many','much', 'wouldn','guy',\n",
    "                             'go', \"wouldn't\",'retard','retards',\n",
    "                             'dick','dickhead','bitch','porn',\n",
    "                             'asshole','pussy','cock']\n",
    "\n",
    "commodities_of_interests=['wheat','soybean','corn','milk','cheese',\n",
    "                          'butter','whey','lean hog','live cattle',\n",
    "                          'lumber','pork','cocoa','sugar','coffee',\n",
    "                          'cotton','diesel','ethanol','natural gas',\n",
    "                          'coal','gasoline','gasoil','methanol',\n",
    "                          'urea','rough rice','oats','palm oil',\n",
    "                          'lead','carbon','greenhouse gas','freight',\n",
    "                          'baltic','iron ore','steel','aluminium',\n",
    "                          'aluminum','copper','gold','silver','brent',\n",
    "                          'wti','henry hub','uranium','cobalt',\n",
    "                          'nickel','zinc','palladium','platinum',\n",
    "                          'propane','naphtha','fuel oil']\n",
    "\n",
    "\n",
    "\n",
    "def scraping_data(session):\n",
    "    \"\"\"scraping\"\"\"\n",
    "    \n",
    "    logger = logging.getLogger('scraping starts')\n",
    "    \n",
    "    flairs=['DD','Discussion',\n",
    "            'Chart','YOLO',\n",
    "            '\"Earnings%20Thread\"',\n",
    "           'Gain','Loss','News']\n",
    "\n",
    "    threads=[]\n",
    "    pages={}\n",
    "\n",
    "    for flair in flairs:\n",
    "        url=f'https://new.reddit.com/r/wallstreetbets/search?sort=hot&restrict_sr=on&q=flair%3A{flair}&t=day'\n",
    "        \n",
    "        logger.debug(f'scraping {flair}')\n",
    "        time.sleep(5)\n",
    "        response=session.get(url,verify=False)\n",
    "\n",
    "        page=bs(response.content,'html.parser')\n",
    "\n",
    "        pages[url]=page\n",
    "        threads+=[i.text for i in page.find_all('span', attrs={'style':\"font-weight:normal\"})]\n",
    "\n",
    "    return threads\n",
    "\n",
    "\n",
    "def create_wordcloud(text):\n",
    "    \"\"\"draw wordcloud\"\"\"\n",
    "    \n",
    "    #use shape    \n",
    "    mask=np.array(Image.open('silhouette.jpg'))\n",
    "\n",
    "    wordcloud=WordCloud(mask=mask,\n",
    "                        #to draw the boundary\n",
    "                        #contour_width=3,contour_color='grey',\n",
    "                        background_color='white',\n",
    "                        #color_func=image_colors,\n",
    "                        colormap='gist_heat',\n",
    "                        stopwords=stopword_dict,\n",
    "                        height=900,\n",
    "                        width=1200,\n",
    "                         ).generate(text)\n",
    "\n",
    "    ax=plt.figure(figsize=(12,9)).add_subplot(111)\n",
    "\n",
    "    #display the image of word cloud\n",
    "    plt.imshow(wordcloud)\n",
    "\n",
    "    #remove axis\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('output.png')\n",
    "    \n",
    "    \n",
    "def create_df_from_dict(potential):\n",
    "    \"\"\"create df from dict\"\"\"\n",
    "    \n",
    "    if len(potential)==0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    #make sure each value has the same length\n",
    "    maxlen=max([len(potential[i]) for i in potential])\n",
    "\n",
    "    for i in potential:\n",
    "        if len(potential[i])!=maxlen:\n",
    "            potential[i]+=['']*(maxlen-len(potential[i]))\n",
    "\n",
    "    return pd.DataFrame().from_dict(potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "\n",
    "session=requests.Session()\n",
    "threads=scraping_data(session)\n",
    "\n",
    "logger.debug(\"prepare for wordcloud\")\n",
    "\n",
    "#etl\n",
    "rawtext=''.join(threads)\n",
    "cleantext=[i for i in rawtext.split(' ') if i.lower() not in stopword_dict]\n",
    "\n",
    "#cleanse\n",
    "potential_tickers={}\n",
    "potential_commodities={}\n",
    "\n",
    "for ind,val in enumerate(cleantext):\n",
    "    \n",
    "    #remove punctuations\n",
    "    for j in punctuations:\n",
    "        if j in val:\n",
    "            cleantext[ind]=val.replace(j,'')\n",
    "            \n",
    "    #remove stopword\n",
    "    if cleantext[ind].lower() in stopword_dict:\n",
    "        cleantext[ind]=''\n",
    "    \n",
    "    #ticker starts with $\n",
    "    if val[0]=='$' and not val[1].isdigit():\n",
    "        potential_tickers[val]=[]\n",
    "    \n",
    "    #find commodities of interests\n",
    "    for ii in commodities_of_interests:\n",
    "        if ii in val.lower():\n",
    "            potential_commodities[ii]=[]\n",
    "\n",
    "#find the context\n",
    "for ind,val in enumerate(threads):\n",
    "    \n",
    "    for j in potential_commodities:\n",
    "        if j in val.lower():\n",
    "            potential_commodities[j].append(val)\n",
    "        \n",
    "    for j in potential_tickers:\n",
    "        if j in val:\n",
    "            potential_tickers[j].append(val)\n",
    "\n",
    "\n",
    "logger.debug(\"create output\")\n",
    "\n",
    "#count freq\n",
    "lexicons=set([i for i in cleantext])\n",
    "D={}\n",
    "for word in lexicons:\n",
    "    D[word]=cleantext.count(word)\n",
    "\n",
    "#create wordcount\n",
    "df=pd.DataFrame()\n",
    "df['word']=D.keys()\n",
    "df['count']=D.values()\n",
    "df.sort_values('count',inplace=True,ascending=False)\n",
    "\n",
    "#create context finder\n",
    "df_commodities=create_df_from_dict(potential_commodities)\n",
    "df_tickers=create_df_from_dict(potential_tickers)\n",
    "\n",
    "#concatenate\n",
    "writer=pd.ExcelWriter('output.xlsx')    \n",
    "df_commodities.to_excel(writer,\n",
    "                        sheet_name='potential commodities',\n",
    "                        index=False)    \n",
    "df_tickers.to_excel(writer,sheet_name='potential tickers',\n",
    "                    index=False)   \n",
    "df.to_excel(writer,sheet_name='word count',\n",
    "                        index=False)    \n",
    "writer.save()\n",
    "\n",
    "\n",
    "logger.debug(\"wordcloud\")    \n",
    "processed=' '.join(cleantext)\n",
    "create_wordcloud(processed)\n",
    "\n",
    "#cleanse text\n",
    "text_commodities=', '.join([i.title() for i in potential_commodities])\n",
    "text_tickers=', '.join([i.upper() for i in potential_tickers])\n",
    "\n",
    "#create html\n",
    "row1=f\"\"\"*Commodities Mentioned: <font color=\"red\">{text_commodities}</font>\"\"\"\n",
    "row2=f\"\"\"*Tickers Mentioned: <font color=\"red\">{text_tickers}</font>\"\"\"\n",
    "disclaimer='*Please check the spreadsheet attached for the exact context of the mentioning.'\n",
    "image=\"\"\"<img width=800 height=600 id=\"1\" src=\"cid:output.png\">\"\"\"\n",
    "html=f\"\"\"<p>{row1}</p><p>{row2}</p><br>{image}<br><p>{disclaimer}</p>\"\"\"\n",
    "\n",
    "\n",
    "files=['output.png','output.xlsx']\n",
    "\n",
    "#send email\n",
    "try:\n",
    "    title = dt.datetime.today()\n",
    "\n",
    "    outlook = win32.Dispatch('outlook.application')  \n",
    "    mail = outlook.CreateItem(0)      \n",
    "    receivers = ['lana.rhodes@brazzers.com',\n",
    "                    'tori.black@brazzers.com',\n",
    "                    'naomi.woods@brazzers.com']  \n",
    "    mail.To = ';'.join(receivers)   \n",
    "    mail.Attachments.Add(Source=files)\n",
    "    mail.Subject ='What was Reddit talking about %s'%(title)\n",
    "    mail.BodyFormat=2    \n",
    "    mail.HTMLBody=html\n",
    "    mail.Send()\n",
    "    \n",
    "except Exception:\n",
    "    print(traceback.format_exc())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
